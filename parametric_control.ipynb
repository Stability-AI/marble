{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1976c7-5491-4124-94c1-dd99b5fcd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLControlNetInpaintPipeline, ControlNetModel\n",
    "from rembg import remove\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ip_adapter_instantstyle import IPAdapterXL\n",
    "from ip_adapter_instantstyle.utils import register_cross_attention_hook, get_net_attn_map, attnmaps2images\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"Import DPT for Depth Model\"\"\"\n",
    "import DPT.util.io\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from DPT.dpt.models import DPTDepthModel\n",
    "from DPT.dpt.midas_net import MidasNet_large\n",
    "from DPT.dpt.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "from parametric_control_mlp import control_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306a004-ac0e-4714-9868-ae77b699fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metallic MLP\n",
    "mlp = control_mlp(1024)\n",
    "mlp.load_state_dict(torch.load('model_weights/metallic.pt'))\n",
    "mlp = mlp.to(\"cuda\", dtype=torch.float16)\n",
    "mlp.eval()\n",
    "\n",
    "# Roughness MLP\n",
    "mlp2 = control_mlp(1024)\n",
    "mlp2.load_state_dict(torch.load('model_weights/roughness.pt'))\n",
    "mlp2 = mlp2.to(\"cuda\", dtype=torch.float16)\n",
    "mlp2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b02bd-42af-4139-8c8e-6a5366be7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get MARBLE Model ready\"\"\"\n",
    "base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"models/image_encoder\"\n",
    "ip_ckpt = \"sdxl_models/ip-adapter_sdxl_vit-h.bin\"\n",
    "controlnet_path = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\"\"\"Load IP-Adapter + Instant Style + Editing MLP\"\"\"\n",
    "cur_block = ('up', 0, 1)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    add_watermarker=False,\n",
    ").to(device)\n",
    "\n",
    "pipe.unet = register_cross_attention_hook(pipe.unet)\n",
    "block_name = cur_block[0] + \"_blocks.\" + str(cur_block[1])+ \".attentions.\" + str(cur_block[2])\n",
    "print(\"Testing block {}\".format(block_name))\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device, target_blocks=[block_name], edit_mlp=mlp, edit_mlp2=mlp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a744532-efde-4949-bffc-9b8d73c88aa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get Depth Model Ready\n",
    "\"\"\"\n",
    "import cv2\n",
    "model_path = \"DPT/dpt_weights/dpt_hybrid-midas-501f0c75.pt\"\n",
    "net_w = net_h = 384\n",
    "model = DPTDepthModel(\n",
    "    path=model_path,\n",
    "    backbone=\"vitb_rn50_384\",\n",
    "    non_negative=True,\n",
    "    enable_attention_hooks=False,\n",
    ")\n",
    "normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "transform = Compose(\n",
    "        [\n",
    "            Resize(\n",
    "                net_w,\n",
    "                net_h,\n",
    "                resize_target=None,\n",
    "                keep_aspect_ratio=True,\n",
    "                ensure_multiple_of=32,\n",
    "                resize_method=\"minimal\",\n",
    "                image_interpolation_method=cv2.INTER_CUBIC,\n",
    "            ),\n",
    "            normalization,\n",
    "            PrepareForNet(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94851e56-fe11-4aa4-8574-e12d0433508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit strengths for metallic. More negative = more metallic, best results between range -20 to 20\n",
    "edit_strengths1 = [-20, 0, 20]\n",
    "\n",
    "# Edit strengths for roughness. More positive = more roughness, best results between range -1 to 1\n",
    "edit_strengths2 = [-1, 0, 1]\n",
    "\n",
    "\n",
    "all_images = []\n",
    "for edit_strength1 in edit_strengths1:\n",
    "    for edit_strength2 in edit_strengths2:\n",
    "        \n",
    "        target_image_path = 'input_images/context_image/toy_car.png'\n",
    "        target_image = Image.open(target_image_path).convert('RGB')\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute depth map from input_image\n",
    "        \"\"\"\n",
    "\n",
    "        img = np.array(target_image)\n",
    "\n",
    "        img_input = transform({\"image\": img})[\"image\"]\n",
    "\n",
    "        # compute\n",
    "        with torch.no_grad():\n",
    "            sample = torch.from_numpy(img_input).unsqueeze(0)\n",
    "\n",
    "            # if optimize == True and device == torch.device(\"cuda\"):\n",
    "            #     sample = sample.to(memory_format=torch.channels_last)\n",
    "            #     sample = sample.half()\n",
    "\n",
    "            prediction = model.forward(sample)\n",
    "            prediction = (\n",
    "                torch.nn.functional.interpolate(\n",
    "                    prediction.unsqueeze(1),\n",
    "                    size=img.shape[:2],\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                )\n",
    "                .squeeze()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        depth_min = prediction.min()\n",
    "        depth_max = prediction.max()\n",
    "        bits = 2\n",
    "        max_val = (2 ** (8 * bits)) - 1\n",
    "\n",
    "        if depth_max - depth_min > np.finfo(\"float\").eps:\n",
    "            out = max_val * (prediction - depth_min) / (depth_max - depth_min)\n",
    "        else:\n",
    "            out = np.zeros(prediction.shape, dtype=depth.dtype)\n",
    "\n",
    "        out = (out / 256).astype('uint8')\n",
    "        depth_map = Image.fromarray(out).resize((1024, 1024))\n",
    "        \n",
    "        \n",
    "        \"\"\"Preprocessing data for MARBLE\"\"\"\n",
    "        rm_bg = remove(target_image)\n",
    "        target_mask = rm_bg.convert(\"RGB\").point(lambda x: 0 if x < 1 else 255).convert('L').convert('RGB')# Convert mask to grayscale\n",
    "\n",
    "        noise = np.random.randint(0, 256, target_image.size + (3,), dtype=np.uint8)\n",
    "        noise_image = Image.fromarray(noise)\n",
    "        mask_target_img = ImageChops.lighter(target_image, target_mask)\n",
    "        invert_target_mask = ImageChops.invert(target_mask)\n",
    "\n",
    "        from PIL import ImageEnhance\n",
    "        gray_target_image = target_image.convert('L').convert('RGB')\n",
    "        gray_target_image = ImageEnhance.Brightness(gray_target_image)\n",
    "\n",
    "        # Adjust brightness\n",
    "        # The factor 1.0 means original brightness, greater than 1.0 makes the image brighter. Adjust this if the image is too dim\n",
    "        factor = 1.0  # Try adjusting this to get the desired brightness\n",
    "\n",
    "        gray_target_image = gray_target_image.enhance(factor)\n",
    "        grayscale_img = ImageChops.darker(gray_target_image, target_mask)\n",
    "        img_black_mask = ImageChops.darker(target_image, invert_target_mask)\n",
    "        grayscale_init_img = ImageChops.lighter(img_black_mask, grayscale_img)\n",
    "        init_img = grayscale_init_img\n",
    "        \n",
    "        # The texture to be applied onto car\n",
    "        ip_image = Image.open('input_images/texture/metal_bowl.png')\n",
    "\n",
    "\n",
    "        init_img = target_image\n",
    "        init_img = init_img.resize((1024,1024))\n",
    "        mask = target_mask.resize((1024, 1024))\n",
    "\n",
    "\n",
    "        cur_seed = 42\n",
    "        images = ip_model.generate_edit_mlp_lr_multi(pil_image = ip_image, image=init_img, control_image=depth_map, \\\n",
    "                                                     mask_image=mask, controlnet_conditioning_scale=1., num_samples=1, \\\n",
    "                                                     num_inference_steps=30, seed=cur_seed, edit_strength=edit_strength1, \\\n",
    "                                                     edit_strength2=edit_strength2, strength=1)\n",
    "        all_images.append(images[0].resize((512,512)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d15b6-efe6-49fb-a33c-589de9781a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image_grid(images, x, y, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Display a list of images in an x by y grid.\n",
    "\n",
    "    Args:\n",
    "        images (list of np.array): List of images (e.g., numpy arrays).\n",
    "        x (int): Number of columns.\n",
    "        y (int): Number of rows.\n",
    "        figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(y, x, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(x * y):\n",
    "        ax = axes[i]\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')  # Hide unused subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "show_image_grid(all_images, len(edit_strengths1), len(edit_strengths2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b21813-7d4c-4f65-964b-c01ea1b21a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
